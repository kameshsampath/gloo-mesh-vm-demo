{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"In my blog I discussed on a technique on how to make your VMS talk to Kubernetes Services over cluster IP and service IP i.e. without using Load Balancers. In this demo I extend that technique to Isito and explore how to onboard Virtual Machines( VM ) workloads. We also go a step further in by configuring few Service Mesh operations like Traffic Splitting, Access Policies using Gloo Mesh which will span both Kubernetes and Virtual Machine services. Demo Architecture","title":"Overview"},{"location":"deploy_apps_on_k8s/","text":"We now have the required environment Kubernetes, Gloo and Istio setup, as part of this chapter let us setup the services that will interact. For this demo we will use three simple microservices namely customer , preference and recommendation ; customer and preference services will be deployed on the Kubernetes cluster1 and the recommendation will be on the vm1 . Demo Service Interactions All the applications used this demo are available as helm charts in my GitHub repo , let us add that helm repo, helm repo add istio-demo-apps https://github.com/kameshsampath/istio-demo-apps helm repo update Now doing a helm search repo istio-demo-apps should show the following apps, NAME CHART VERSION APP VERSION DESCRIPTION istio-demo-apps/customer 0.1.0 1.0.0 A Helm chart to deploy customer demo application istio-demo-apps/preference 0.1.0 1.0.0 A Helm chart to deploy Preference demo application istio-demo-apps/recommendation 0.1.0 1.0.0 A Helm chart for A Helm chart to deploy recommendation demo application Enable automatic sidecar injection on the default namespace, kubectl label ns default istio.io/rev = 1 -11-5 --overwrite Important Since we use revision based we use istio.io/rev label. If you have already labelled the namespace with istio-injection=enabled then remove it by, kubectl label ns default istio-injection- Deploy customer service, helm install --kube-context = \" $CLUSTER1 \" \\ customer istio-demo-apps/customer \\ --set enableIstioGateway = \"true\" Resources \u00b6 Pods and Services \u00b6 kubectl --context = \" $CLUSTER1 \" get svc,pods NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/kubernetes ClusterIP 172 .18.0.1 <none> 443 /TCP 39h service/customer ClusterIP 172 .18.7.18 <none> 8080 /TCP 3m9s NAME READY STATUS RESTARTS AGE pod/customer-59f5854d89-cm84k 2 /2 Running 0 3m9s Gateway and Virtual Services \u00b6 kubectl --context = \" $CLUSTER1 \" get gw,svc NAME AGE gateway.networking.istio.io/customer-gateway 4m57s NAME GATEWAYS HOSTS AGE virtualservice.networking.istio.io/customer-v1-vs [\"customer-gateway\"] [\"*\"] 4m57s Interacting with Services \u00b6 Retrieve the Istio Ingress Gateway url to access the application, export INGRESS_GATEWAY_IP = $( kubectl --context ${ CLUSTER1 } -n istio-gateways get svc ingressgateway -o jsonpath = '{.status.loadBalancer.ingress[0].*}' ) export SVC_URL = \" ${ INGRESS_GATEWAY_IP } /customer\" Call Service \u00b6 Call the service using the script, $DEMO_HOME /bin/call_service.sh Poll Service \u00b6 Poll the service using the script, $DEMO_HOME /bin/poll_service.sh Using Default Browser \u00b6 Open the URL in the browser open http://$SVC_URL . The service calls will fail with error, customer => Service host 'http://preference:8080' not known. Let us deploy preference service, helm install --kube-context = \" $CLUSTER1 \" preference istio-demo-apps/preference Now getting pods and services should show the following output, kubectl --context = \" $CLUSTER1 \" get svc,pods NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/kubernetes ClusterIP 172 .18.0.1 <none> 443 /TCP 39h service/customer ClusterIP 172 .18.7.18 <none> 8080 /TCP 30m service/preference ClusterIP 172 .18.6.50 <none> 8080 /TCP 73s NAME READY STATUS RESTARTS AGE pod/customer-6bbb45d7db-5db6l 2 /2 Running 0 11m pod/preference-b95d64c99-9wjfp 2 /2 Running 0 72s Now calling the customer service again, curl $SVC_URL The command should shown an output like, customer => preference => Service host 'http://recommendation:8080' not known.% Excellent! We not got our Kubernetes services ready. In next chapter lets deploy the recommendation service and make services to work together.","title":"Deploy applications on Kubernetes"},{"location":"deploy_apps_on_k8s/#resources","text":"","title":"Resources"},{"location":"deploy_apps_on_k8s/#pods-and-services","text":"kubectl --context = \" $CLUSTER1 \" get svc,pods NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/kubernetes ClusterIP 172 .18.0.1 <none> 443 /TCP 39h service/customer ClusterIP 172 .18.7.18 <none> 8080 /TCP 3m9s NAME READY STATUS RESTARTS AGE pod/customer-59f5854d89-cm84k 2 /2 Running 0 3m9s","title":"Pods and Services"},{"location":"deploy_apps_on_k8s/#gateway-and-virtual-services","text":"kubectl --context = \" $CLUSTER1 \" get gw,svc NAME AGE gateway.networking.istio.io/customer-gateway 4m57s NAME GATEWAYS HOSTS AGE virtualservice.networking.istio.io/customer-v1-vs [\"customer-gateway\"] [\"*\"] 4m57s","title":"Gateway and Virtual Services"},{"location":"deploy_apps_on_k8s/#interacting-with-services","text":"Retrieve the Istio Ingress Gateway url to access the application, export INGRESS_GATEWAY_IP = $( kubectl --context ${ CLUSTER1 } -n istio-gateways get svc ingressgateway -o jsonpath = '{.status.loadBalancer.ingress[0].*}' ) export SVC_URL = \" ${ INGRESS_GATEWAY_IP } /customer\"","title":"Interacting with Services"},{"location":"deploy_apps_on_k8s/#call-service","text":"Call the service using the script, $DEMO_HOME /bin/call_service.sh","title":"Call Service"},{"location":"deploy_apps_on_k8s/#poll-service","text":"Poll the service using the script, $DEMO_HOME /bin/poll_service.sh","title":"Poll Service"},{"location":"deploy_apps_on_k8s/#using-default-browser","text":"Open the URL in the browser open http://$SVC_URL . The service calls will fail with error, customer => Service host 'http://preference:8080' not known. Let us deploy preference service, helm install --kube-context = \" $CLUSTER1 \" preference istio-demo-apps/preference Now getting pods and services should show the following output, kubectl --context = \" $CLUSTER1 \" get svc,pods NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/kubernetes ClusterIP 172 .18.0.1 <none> 443 /TCP 39h service/customer ClusterIP 172 .18.7.18 <none> 8080 /TCP 30m service/preference ClusterIP 172 .18.6.50 <none> 8080 /TCP 73s NAME READY STATUS RESTARTS AGE pod/customer-6bbb45d7db-5db6l 2 /2 Running 0 11m pod/preference-b95d64c99-9wjfp 2 /2 Running 0 72s Now calling the customer service again, curl $SVC_URL The command should shown an output like, customer => preference => Service host 'http://recommendation:8080' not known.% Excellent! We not got our Kubernetes services ready. In next chapter lets deploy the recommendation service and make services to work together.","title":"Using Default Browser"},{"location":"deploy_apps_on_vm/","text":"In the previous chapter we deployed the customer and preference service on to cluster1 Kubernetes cluster. To complete our demo services interaction let us deploy recommendation on the VM . As there are lot of moving parts 1 in doing this setup, I packed them into set of Ansible tasks . When you run the tasks of the playbook, it will do all that is required setup Istio Sidecar and recommendation namely, Recommendation v1 service running in vm as daemon Istio Sidecar installed and configured on the VM IP Routes from VM to Kubernetes Service and Pods. Check Istio VM Prerequisites for more details. Create a gateway so that the VM Istio Sidecar (Envoy) can communicate securely ( mTLS ) with istiod on cluster1 . At end of this chapter we should be able to make the service interaction flow work as shown below. Demo Service Interactions Run the following command to prepare the VM with required packages, make deploy-base Then running the following command will make the VM ready with recommendation app with Istio Sidecar, make deploy-workload Resources \u00b6 Pods and Services \u00b6 kubectl --context = \" $CLUSTER1 \" get svc,pods NAME READY STATUS RESTARTS AGE pod/customer-6bbb45d7db-5db6l 2 /2 Running 0 56m pod/preference-b95d64c99-9wjfp 2 /2 Running 0 46m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/kubernetes ClusterIP 172 .18.0.1 <none> 443 /TCP 40h service/customer ClusterIP 172 .18.7.18 <none> 8080 /TCP 75m service/preference ClusterIP 172 .18.6.50 <none> 8080 /TCP 46m service/recommendation ClusterIP 172 .18.6.63 <none> 8080 /TCP 2m19s Workload Entry \u00b6 A recommendation Workload Entry , kubectl --context = \" $CLUSTER1 \" workloadentries.networking.istio.io -owide Name : recommendation-vm1 Namespace : default Labels : <none> Annotations : <none> API Version : networking.istio.io/v1beta1 Kind : WorkloadEntry Spec : Address : 192.168.64.92 Labels : App : recommendation instance_id : vm1 Version : v1 Network : network1 Service Account : vm-service-account Events : <none> As you noticed in the above output the output the address of the WorkloadEntry points to the vm1 IP address. The recommendation service will use the selector labels from the WorkloadEntry i.e App to route the traffic to the vm1 using the IP Address from WorkloadEntry. Note You can get vm1 details using the command multipass info vm1 Interacting with Services \u00b6 Retrieve the Istio Ingress Gateway url to access the application, export INGRESS_GATEWAY_IP = $( kubectl --context ${ CLUSTER1 } -n istio-gateways get svc ingressgateway -o jsonpath = '{.status.loadBalancer.ingress[0].*}' ) export SVC_URL = \" ${ INGRESS_GATEWAY_IP } /customer\" Call Service \u00b6 Call the service using the script, $DEMO_HOME /bin/call_service.sh Poll Service \u00b6 Poll the service using the script, $DEMO_HOME /bin/poll_service.sh Using Default Browser \u00b6 Open the URL in the browser open http://$SVC_URL . The customer service should now succeed completing the call to recommendation service on the vm1 , customer => preference => recommendation v1 from 'vm1': 1 Voil\u00e0! We have now successfully connected all the services right from Kubernetes to VM . The communication from vm to k8s should also work, lets do a quick test calling same customer service, multipass exec vm1 -- curl customer.default.svc.cluster.local:8080 And you should get pretty much the same response, customer => preference => recommendation v1 from 'vm1': 2 In next chapter we will see how to apply various Service Mesh features like Rate Limit, Access Policies etc., using Gloo Mesh. https://istio.io/v1.11/docs/setup/install/virtual-machine \u21a9","title":"Deploy application on VM"},{"location":"deploy_apps_on_vm/#resources","text":"","title":"Resources"},{"location":"deploy_apps_on_vm/#pods-and-services","text":"kubectl --context = \" $CLUSTER1 \" get svc,pods NAME READY STATUS RESTARTS AGE pod/customer-6bbb45d7db-5db6l 2 /2 Running 0 56m pod/preference-b95d64c99-9wjfp 2 /2 Running 0 46m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/kubernetes ClusterIP 172 .18.0.1 <none> 443 /TCP 40h service/customer ClusterIP 172 .18.7.18 <none> 8080 /TCP 75m service/preference ClusterIP 172 .18.6.50 <none> 8080 /TCP 46m service/recommendation ClusterIP 172 .18.6.63 <none> 8080 /TCP 2m19s","title":"Pods and Services"},{"location":"deploy_apps_on_vm/#workload-entry","text":"A recommendation Workload Entry , kubectl --context = \" $CLUSTER1 \" workloadentries.networking.istio.io -owide Name : recommendation-vm1 Namespace : default Labels : <none> Annotations : <none> API Version : networking.istio.io/v1beta1 Kind : WorkloadEntry Spec : Address : 192.168.64.92 Labels : App : recommendation instance_id : vm1 Version : v1 Network : network1 Service Account : vm-service-account Events : <none> As you noticed in the above output the output the address of the WorkloadEntry points to the vm1 IP address. The recommendation service will use the selector labels from the WorkloadEntry i.e App to route the traffic to the vm1 using the IP Address from WorkloadEntry. Note You can get vm1 details using the command multipass info vm1","title":"Workload Entry"},{"location":"deploy_apps_on_vm/#interacting-with-services","text":"Retrieve the Istio Ingress Gateway url to access the application, export INGRESS_GATEWAY_IP = $( kubectl --context ${ CLUSTER1 } -n istio-gateways get svc ingressgateway -o jsonpath = '{.status.loadBalancer.ingress[0].*}' ) export SVC_URL = \" ${ INGRESS_GATEWAY_IP } /customer\"","title":"Interacting with Services"},{"location":"deploy_apps_on_vm/#call-service","text":"Call the service using the script, $DEMO_HOME /bin/call_service.sh","title":"Call Service"},{"location":"deploy_apps_on_vm/#poll-service","text":"Poll the service using the script, $DEMO_HOME /bin/poll_service.sh","title":"Poll Service"},{"location":"deploy_apps_on_vm/#using-default-browser","text":"Open the URL in the browser open http://$SVC_URL . The customer service should now succeed completing the call to recommendation service on the vm1 , customer => preference => recommendation v1 from 'vm1': 1 Voil\u00e0! We have now successfully connected all the services right from Kubernetes to VM . The communication from vm to k8s should also work, lets do a quick test calling same customer service, multipass exec vm1 -- curl customer.default.svc.cluster.local:8080 And you should get pretty much the same response, customer => preference => recommendation v1 from 'vm1': 2 In next chapter we will see how to apply various Service Mesh features like Rate Limit, Access Policies etc., using Gloo Mesh. https://istio.io/v1.11/docs/setup/install/virtual-machine \u21a9","title":"Using Default Browser"},{"location":"role_k3s/","text":"This role helps in installing and configuring Argocd the Kubernetes cluster. Requirements \u00b6 Access to Kubernetes cluster, Variables \u00b6 Name Description Default kubernetes_spices_argocd_k8s_context The Kubernetes context where Argcod will be installed. The playbook will fail if this is not set. kubernetes_spices_argocd_namespace The namespace to install Argocd argocd kubernetes_spices_argocd_version The argocd version to be used 2.1.6 kubernetes_spices_argocd_helm_secerts_plugin Use helm secrets plugin with argocd applications false Example Playbook \u00b6 Important Based on the above example the kubernetes_spices_argocd_k8s_context should be set to mgmt , the context which is created by minikube The default credentials to access argocd will be admin/password Using helm secrets plugin \u00b6 To use helm secrets plugin with Argocd applications, enable the plugin configuration by adding enable the flag kubernetes_spices_argocd_helm_secerts_plugin to true . Lets take an example of sops and age , Create age key \u00b6 age-keygen -o key.txt Move the key.txt to secure place, preferably $HOME/.ssh . Assuming you moved it to $HOME/.ssh , lets set that as local environment variables for convinience: export SOPS_AGE_KEY_FILE = \" $HOME /.ssh/key.txt\" Also note and export the publickey in the $SOPS_AGE_KEY_FILE as $SOPS_AGE_RECIPIENTS export SOPS_AGE_RECIPIENTS = $( cat $SOPS_AGE_KEY_FILE | awk 'NR==2{ print $4}' ) Ensure the sops configration .sops.yml is updated with your age publickey, yq eval '.creation_rules[0].age |= strenv(SOPS_AGE_RECIPIENTS)' .sops.yml We need to make the age key to be available to the Argocd repo server so that it can decrypt the secrets, kubectl create ns argocd kubectl create secret generic helm-secrets-private-keys \\ --namespace = argocd \\ --from-file = key.txt = \" $SOPS_AGE_KEY_FILE \" Now you an use the same play to deploy Argocd with helm secrets enabled, You can check the example project to deploy Keycloak using helm secrets plugin enabled with Argocd.","title":"k3s"},{"location":"role_k3s/#requirements","text":"Access to Kubernetes cluster,","title":"Requirements"},{"location":"role_k3s/#variables","text":"Name Description Default kubernetes_spices_argocd_k8s_context The Kubernetes context where Argcod will be installed. The playbook will fail if this is not set. kubernetes_spices_argocd_namespace The namespace to install Argocd argocd kubernetes_spices_argocd_version The argocd version to be used 2.1.6 kubernetes_spices_argocd_helm_secerts_plugin Use helm secrets plugin with argocd applications false","title":"Variables"},{"location":"role_k3s/#example-playbook","text":"Important Based on the above example the kubernetes_spices_argocd_k8s_context should be set to mgmt , the context which is created by minikube The default credentials to access argocd will be admin/password","title":"Example Playbook"},{"location":"role_k3s/#using-helm-secrets-plugin","text":"To use helm secrets plugin with Argocd applications, enable the plugin configuration by adding enable the flag kubernetes_spices_argocd_helm_secerts_plugin to true . Lets take an example of sops and age ,","title":"Using helm secrets plugin"},{"location":"role_k3s/#create-age-key","text":"age-keygen -o key.txt Move the key.txt to secure place, preferably $HOME/.ssh . Assuming you moved it to $HOME/.ssh , lets set that as local environment variables for convinience: export SOPS_AGE_KEY_FILE = \" $HOME /.ssh/key.txt\" Also note and export the publickey in the $SOPS_AGE_KEY_FILE as $SOPS_AGE_RECIPIENTS export SOPS_AGE_RECIPIENTS = $( cat $SOPS_AGE_KEY_FILE | awk 'NR==2{ print $4}' ) Ensure the sops configration .sops.yml is updated with your age publickey, yq eval '.creation_rules[0].age |= strenv(SOPS_AGE_RECIPIENTS)' .sops.yml We need to make the age key to be available to the Argocd repo server so that it can decrypt the secrets, kubectl create ns argocd kubectl create secret generic helm-secrets-private-keys \\ --namespace = argocd \\ --from-file = key.txt = \" $SOPS_AGE_KEY_FILE \" Now you an use the same play to deploy Argocd with helm secrets enabled, You can check the example project to deploy Keycloak using helm secrets plugin enabled with Argocd.","title":"Create age key"},{"location":"role_multipass/","text":"This role helps in installing and configuring Argocd the Kubernetes cluster. Requirements \u00b6 Access to Kubernetes cluster, Variables \u00b6 Name Description Default kubernetes_spices_argocd_k8s_context The Kubernetes context where Argcod will be installed. The playbook will fail if this is not set. kubernetes_spices_argocd_namespace The namespace to install Argocd argocd kubernetes_spices_argocd_version The argocd version to be used 2.1.6 kubernetes_spices_argocd_helm_secerts_plugin Use helm secrets plugin with argocd applications false Example Playbook \u00b6 Important Based on the above example the kubernetes_spices_argocd_k8s_context should be set to mgmt , the context which is created by minikube The default credentials to access argocd will be admin/password Using helm secrets plugin \u00b6 To use helm secrets plugin with Argocd applications, enable the plugin configuration by adding enable the flag kubernetes_spices_argocd_helm_secerts_plugin to true . Lets take an example of sops and age , Create age key \u00b6 age-keygen -o key.txt Move the key.txt to secure place, preferably $HOME/.ssh . Assuming you moved it to $HOME/.ssh , lets set that as local environment variables for convinience: export SOPS_AGE_KEY_FILE = \" $HOME /.ssh/key.txt\" Also note and export the publickey in the $SOPS_AGE_KEY_FILE as $SOPS_AGE_RECIPIENTS export SOPS_AGE_RECIPIENTS = $( cat $SOPS_AGE_KEY_FILE | awk 'NR==2{ print $4}' ) Ensure the sops configration .sops.yml is updated with your age publickey, yq eval '.creation_rules[0].age |= strenv(SOPS_AGE_RECIPIENTS)' .sops.yml We need to make the age key to be available to the Argocd repo server so that it can decrypt the secrets, kubectl create ns argocd kubectl create secret generic helm-secrets-private-keys \\ --namespace = argocd \\ --from-file = key.txt = \" $SOPS_AGE_KEY_FILE \" Now you an use the same play to deploy Argocd with helm secrets enabled, You can check the example project to deploy Keycloak using helm secrets plugin enabled with Argocd.","title":"Multipass"},{"location":"role_multipass/#requirements","text":"Access to Kubernetes cluster,","title":"Requirements"},{"location":"role_multipass/#variables","text":"Name Description Default kubernetes_spices_argocd_k8s_context The Kubernetes context where Argcod will be installed. The playbook will fail if this is not set. kubernetes_spices_argocd_namespace The namespace to install Argocd argocd kubernetes_spices_argocd_version The argocd version to be used 2.1.6 kubernetes_spices_argocd_helm_secerts_plugin Use helm secrets plugin with argocd applications false","title":"Variables"},{"location":"role_multipass/#example-playbook","text":"Important Based on the above example the kubernetes_spices_argocd_k8s_context should be set to mgmt , the context which is created by minikube The default credentials to access argocd will be admin/password","title":"Example Playbook"},{"location":"role_multipass/#using-helm-secrets-plugin","text":"To use helm secrets plugin with Argocd applications, enable the plugin configuration by adding enable the flag kubernetes_spices_argocd_helm_secerts_plugin to true . Lets take an example of sops and age ,","title":"Using helm secrets plugin"},{"location":"role_multipass/#create-age-key","text":"age-keygen -o key.txt Move the key.txt to secure place, preferably $HOME/.ssh . Assuming you moved it to $HOME/.ssh , lets set that as local environment variables for convinience: export SOPS_AGE_KEY_FILE = \" $HOME /.ssh/key.txt\" Also note and export the publickey in the $SOPS_AGE_KEY_FILE as $SOPS_AGE_RECIPIENTS export SOPS_AGE_RECIPIENTS = $( cat $SOPS_AGE_KEY_FILE | awk 'NR==2{ print $4}' ) Ensure the sops configration .sops.yml is updated with your age publickey, yq eval '.creation_rules[0].age |= strenv(SOPS_AGE_RECIPIENTS)' .sops.yml We need to make the age key to be available to the Argocd repo server so that it can decrypt the secrets, kubectl create ns argocd kubectl create secret generic helm-secrets-private-keys \\ --namespace = argocd \\ --from-file = key.txt = \" $SOPS_AGE_KEY_FILE \" Now you an use the same play to deploy Argocd with helm secrets enabled, You can check the example project to deploy Keycloak using helm secrets plugin enabled with Argocd.","title":"Create age key"},{"location":"role_workload_vm/","text":"This role helps in installing and configuring Argocd the Kubernetes cluster. Requirements \u00b6 Access to Kubernetes cluster, Variables \u00b6 Name Description Default kubernetes_spices_argocd_k8s_context The Kubernetes context where Argcod will be installed. The playbook will fail if this is not set. kubernetes_spices_argocd_namespace The namespace to install Argocd argocd kubernetes_spices_argocd_version The argocd version to be used 2.1.6 kubernetes_spices_argocd_helm_secerts_plugin Use helm secrets plugin with argocd applications false Example Playbook \u00b6 Important Based on the above example the kubernetes_spices_argocd_k8s_context should be set to mgmt , the context which is created by minikube The default credentials to access argocd will be admin/password Using helm secrets plugin \u00b6 To use helm secrets plugin with Argocd applications, enable the plugin configuration by adding enable the flag kubernetes_spices_argocd_helm_secerts_plugin to true . Lets take an example of sops and age , Create age key \u00b6 age-keygen -o key.txt Move the key.txt to secure place, preferably $HOME/.ssh . Assuming you moved it to $HOME/.ssh , lets set that as local environment variables for convinience: export SOPS_AGE_KEY_FILE = \" $HOME /.ssh/key.txt\" Also note and export the publickey in the $SOPS_AGE_KEY_FILE as $SOPS_AGE_RECIPIENTS export SOPS_AGE_RECIPIENTS = $( cat $SOPS_AGE_KEY_FILE | awk 'NR==2{ print $4}' ) Ensure the sops configration .sops.yml is updated with your age publickey, yq eval '.creation_rules[0].age |= strenv(SOPS_AGE_RECIPIENTS)' .sops.yml We need to make the age key to be available to the Argocd repo server so that it can decrypt the secrets, kubectl create ns argocd kubectl create secret generic helm-secrets-private-keys \\ --namespace = argocd \\ --from-file = key.txt = \" $SOPS_AGE_KEY_FILE \" Now you an use the same play to deploy Argocd with helm secrets enabled, You can check the example project to deploy Keycloak using helm secrets plugin enabled with Argocd.","title":"Workload VM"},{"location":"role_workload_vm/#requirements","text":"Access to Kubernetes cluster,","title":"Requirements"},{"location":"role_workload_vm/#variables","text":"Name Description Default kubernetes_spices_argocd_k8s_context The Kubernetes context where Argcod will be installed. The playbook will fail if this is not set. kubernetes_spices_argocd_namespace The namespace to install Argocd argocd kubernetes_spices_argocd_version The argocd version to be used 2.1.6 kubernetes_spices_argocd_helm_secerts_plugin Use helm secrets plugin with argocd applications false","title":"Variables"},{"location":"role_workload_vm/#example-playbook","text":"Important Based on the above example the kubernetes_spices_argocd_k8s_context should be set to mgmt , the context which is created by minikube The default credentials to access argocd will be admin/password","title":"Example Playbook"},{"location":"role_workload_vm/#using-helm-secrets-plugin","text":"To use helm secrets plugin with Argocd applications, enable the plugin configuration by adding enable the flag kubernetes_spices_argocd_helm_secerts_plugin to true . Lets take an example of sops and age ,","title":"Using helm secrets plugin"},{"location":"role_workload_vm/#create-age-key","text":"age-keygen -o key.txt Move the key.txt to secure place, preferably $HOME/.ssh . Assuming you moved it to $HOME/.ssh , lets set that as local environment variables for convinience: export SOPS_AGE_KEY_FILE = \" $HOME /.ssh/key.txt\" Also note and export the publickey in the $SOPS_AGE_KEY_FILE as $SOPS_AGE_RECIPIENTS export SOPS_AGE_RECIPIENTS = $( cat $SOPS_AGE_KEY_FILE | awk 'NR==2{ print $4}' ) Ensure the sops configration .sops.yml is updated with your age publickey, yq eval '.creation_rules[0].age |= strenv(SOPS_AGE_RECIPIENTS)' .sops.yml We need to make the age key to be available to the Argocd repo server so that it can decrypt the secrets, kubectl create ns argocd kubectl create secret generic helm-secrets-private-keys \\ --namespace = argocd \\ --from-file = key.txt = \" $SOPS_AGE_KEY_FILE \" Now you an use the same play to deploy Argocd with helm secrets enabled, You can check the example project to deploy Keycloak using helm secrets plugin enabled with Argocd.","title":"Create age key"},{"location":"setup/","text":"At the end of this chapter you would have setup the environment and tools that are required for the demo. Download Tools \u00b6 We will be using the following tools as part of the tutorial. Please have them installed and configured before proceeding further. Tool macos linux windows direnv brew install direnv Install N.A helm brew install helm Package Managers choco install kubernetes-helm multipass brew install multipass Download Download jq brew install jq Install choco install yq kubectl brew install kubectl Download choco install kubernetes-cli Python3 brew install kubectl Download Download stern(optional) brew install stern Download Download Download Sources \u00b6 git clone https://github.com/kameshsampath/gloo-mesh-vm-demo cd gloo-mesh-vm-demo For the rest of the instructions, the cloned sources folder will be referred to as $DEMO_HOME . Ensure Environment \u00b6 Assuming you have downloaded direnv and hooked it to your shell . direnv allow . Setup Ansible Environment \u00b6 The demo will be using Ansible to setup the environment, run the following command to install Ansible modules and extra collections and roles the will be used for various tasks. make setup-ansible Important The demo uses direnv to create a local Python Virtual Environment. Ansible Variables \u00b6 Variable Description Default Value Ansible Role work_dir The work directory on the local machine {{ playbook_dir }}/work kubeconfig_dir The directory to save all the kubeconfig files {{ work_dir }}/.kube kubernetes_cli_version the kubectl version v1.21.8 k3s_cluster_cidr The Cluster CIDR( cluster-cidr ) for Kubernetes Clusters 172.16.0.0/24 k3s k3s_service_cidr The Service CIDR(service-cidr) for Kubernetes Clusters 172.18.0.0/20 k3s istio_enabled Whether to configure the vms with Istio Sidecar yes Workload istio_vm_app VM application name recommendation Workload istio_vm_namespace The namespace in Kubernetes cluster i.e. cluster1 default Workload istio_vm_workdir The work dir in vm where the Istio Sidecar files will be created /home/{{ ansible_user }}/istio-vm/files Workload istio_vm_service_account The Kubernetes Service Account to use when creating VM resources in Kubernetes vm-service-account Workload istio_cluster_network The Istio Cluster Network the network network1 Workload istio_vm_network The Istio network for VM communication Workload istio_cluster The Istio cluster name. The name in this demo maps to Kubernetes cluster context where to install Istio i.e cluster1 and the same is used as SPIFEE trustDomain cluster1 Workload istio_cluster_service_ip_cidr The Cluster Service IP CIDR to use with istio_cluster {{ k3s_service_cidr }} Workload istio_cluster_pod_ip_cidr The Cluster IP CIDR to use with istio_cluster {{ k3s_cluster_cidr }} Workload workload_istio_ns The namespace where Istio Control Plane is deployed in istio_cluster {{ k3s_cluster_cidr }} Workload workload_istio_gateway_ns The namespace where Istio Ingress gateway is deployed in istio_cluster {{ k3s_cluster_cidr }} Workload clean_istio_vm_files Clean the generated Istio sidecar VM files including the directories where it was copied in the VM yes Workload force_app_install Clean install the VM application no Workload Apart from the variables defined, there are three other variables that controls the setup, multipass_vms - defines a dictionary of VMs that needs to be created, multipass_vms : # the name of the VM - name : mgmt # cpus to allocate cpus : 4 # memory to allocate mem : 8g # disk size disk : 30g # roles of this vm role : - kubernetes - gloo - management - name : cluster1 cpus : 4 mem : 8g disk : 30g role : - kubernetes - gloo - workload - name : vm1 cpus : 2 mem : 2g disk : 30g role : - vm gloo_clusters - the Kubernetes clusters that wil be used for gloo deployment gloo_clusters : # name of the cluster mgmt : # cloud where its deployed cloud : k3s # the Kubernetes Context name, recommended it to be the name of VM where k3s runs k8s_context : mgmt # logical cluster name to be used while registering it with meshctl cluster_name : mgmt cluster1 : cloud : k3s k8s_context : cluster1 cluster_name : cluster1 istio_clusters - the Kubernetes clusters where Istio will be deployed istio_clusters : # name of the cluster cluster1 : # Kubernetes Context to use for this cluster k8s_context : \"{{ gloo_clusters.cluster1.k8s_context }}\" # The version of Istio that needs to be deployed version : \"{{ lookup('env','ISTIO_VERSION') }}\" install : yes Tip The demo uses asdf-vm to handle multiple versions of a software e.g. Python, Istio. Check out https://github.com/kameshsampath/asdf-istio The setup uses direnv and the playbooks generates the .envrc using template form $DEMO_HOME/templates/.envrc . If needed adjust the .envrc template and rerun the create-vms and create-kubernetes-clusters task to refresh or update it. Create Virtual Machines \u00b6 For the demo we will be using multipass to create and run virtual machines. Run the following command to create the virtual machines, make create-vms The previous command would have created three VMs namely, mgmt - which will act as Gloo Management Kubernetes cluster. cluster1 - The Kubernetes cluster where Istio and its workloads will be deployed. The Virtual Machine workloads will use the Istio Control Plane( CP ) opn this cluster for its services. vm1 - the Virtual Machine which will hold a small workload that will be connected to cluster1 . You can always get the information about the multipass VM using the command, multipass info <vm name> # e.g. multipass info cluster1 That should give an information like, Name: cluster1 State: Running IPv4: 192.168.64.90 172.16.0.0 Release: Ubuntu 20.04.3 LTS Image hash: 8fbc4e8c6e33 (Ubuntu 20.04 LTS) Load: 2.37 1.88 1.76 Disk usage: 5.3G out of 28.9G Memory usage: 1.8G out of 7.8G Mounts: -- The task finally generates Ansible Hosts inventory based on the template from $DEMO_HOME/templates/hosts.j2 , which will be used as inventory in other playbook runs. Setup Kubernetes Clusters \u00b6 As part of this demo we will be setting up k3s Kubernetes clusters. The k3s clusters will be a single node cluster run via multipass VM . We will configure that to with the following flags, --cluster-cidr=172.16.0.0/24 allows us to create 65 \u2013 110 Pods on this node --service-cidr=172.18.0.0/20 allows us to create 4096 services --disable=traefik disable traefik deployment Check the GKE doc 1 for a reference on how to calculate the number of pods and service with given CIDR range. Run the following command to deploy the clusters to our mgmt and cluster1 VMs. make create-kubernetes-clusters The previous step we should have two Kubernetes clusters mgmt and cluster1 and as convenience it merges the two cluster kubeconfig into one as $DEMO_HOME/work/.kube/config which is set as the current shell $KUBECONFIG value. So doing kubectl config get-contexts now should return you two contexts. CURRENT NAME CLUSTER AUTHINFO NAMESPACE cluster1 cluster1 cluster1 * mgmt mgmt mgmt Note The k3s deployment as part of this demo will be using Calico which enables us to define routes to the Kubernetes services/pods via its host . Setup Gloo \u00b6 Let us setup on the mgmt cluster. The setup uses the Gloo Enterprise License, if you don\u2019t have one please request 30 day trial one via solo.io . Set the License key via as $GLOO_MESH_GATEWAY_LICENSE_KEY environment variable and then run the following command to deploy Gloo Mesh, make deploy-gloo Ensure Gloo mesh is setup correctly, meshctl check server Gloo Mesh Management Cluster Installation -------------------------------------------- \ud83d\udfe2 Gloo Mesh Pods Status +----------+------------+-------------------------------+-----------------+ | CLUSTER | REGISTERED | DASHBOARDS AND AGENTS PULLING | AGENTS PUSHING | +----------+------------+-------------------------------+-----------------+ | cluster1 | true | 2 | 1 | +----------+------------+-------------------------------+-----------------+ \ud83d\udfe2 Gloo Mesh Agents Connectivity Management Configuration --------------------------- \ud83d\udfe2 Gloo Mesh CRD Versions \ud83d\udfe2 Gloo Mesh Networking Configuration Resources Setup Istio \u00b6 Lets now complete the environment setup part by deploying Istio on to cluster1 . We use Istio 1.11.5 for this tutorial, make deploy-istio Lets check if Istio setup is done correctly, istioctl verify-install --context = $CLUSTER1 The command should show the following output ( trimmed for brevity), ... Checked 13 custom resource definitions Checked 1 Istio Deployments \u2714 Istio is installed and verified successfully https://cloud.google.com/kubernetes-engine/docs/concepts/alias-ips \u21a9","title":"Setup"},{"location":"setup/#download-tools","text":"We will be using the following tools as part of the tutorial. Please have them installed and configured before proceeding further. Tool macos linux windows direnv brew install direnv Install N.A helm brew install helm Package Managers choco install kubernetes-helm multipass brew install multipass Download Download jq brew install jq Install choco install yq kubectl brew install kubectl Download choco install kubernetes-cli Python3 brew install kubectl Download Download stern(optional) brew install stern Download Download","title":"Download Tools"},{"location":"setup/#download-sources","text":"git clone https://github.com/kameshsampath/gloo-mesh-vm-demo cd gloo-mesh-vm-demo For the rest of the instructions, the cloned sources folder will be referred to as $DEMO_HOME .","title":"Download Sources"},{"location":"setup/#ensure-environment","text":"Assuming you have downloaded direnv and hooked it to your shell . direnv allow .","title":"Ensure Environment"},{"location":"setup/#setup-ansible-environment","text":"The demo will be using Ansible to setup the environment, run the following command to install Ansible modules and extra collections and roles the will be used for various tasks. make setup-ansible Important The demo uses direnv to create a local Python Virtual Environment.","title":"Setup Ansible Environment"},{"location":"setup/#ansible-variables","text":"Variable Description Default Value Ansible Role work_dir The work directory on the local machine {{ playbook_dir }}/work kubeconfig_dir The directory to save all the kubeconfig files {{ work_dir }}/.kube kubernetes_cli_version the kubectl version v1.21.8 k3s_cluster_cidr The Cluster CIDR( cluster-cidr ) for Kubernetes Clusters 172.16.0.0/24 k3s k3s_service_cidr The Service CIDR(service-cidr) for Kubernetes Clusters 172.18.0.0/20 k3s istio_enabled Whether to configure the vms with Istio Sidecar yes Workload istio_vm_app VM application name recommendation Workload istio_vm_namespace The namespace in Kubernetes cluster i.e. cluster1 default Workload istio_vm_workdir The work dir in vm where the Istio Sidecar files will be created /home/{{ ansible_user }}/istio-vm/files Workload istio_vm_service_account The Kubernetes Service Account to use when creating VM resources in Kubernetes vm-service-account Workload istio_cluster_network The Istio Cluster Network the network network1 Workload istio_vm_network The Istio network for VM communication Workload istio_cluster The Istio cluster name. The name in this demo maps to Kubernetes cluster context where to install Istio i.e cluster1 and the same is used as SPIFEE trustDomain cluster1 Workload istio_cluster_service_ip_cidr The Cluster Service IP CIDR to use with istio_cluster {{ k3s_service_cidr }} Workload istio_cluster_pod_ip_cidr The Cluster IP CIDR to use with istio_cluster {{ k3s_cluster_cidr }} Workload workload_istio_ns The namespace where Istio Control Plane is deployed in istio_cluster {{ k3s_cluster_cidr }} Workload workload_istio_gateway_ns The namespace where Istio Ingress gateway is deployed in istio_cluster {{ k3s_cluster_cidr }} Workload clean_istio_vm_files Clean the generated Istio sidecar VM files including the directories where it was copied in the VM yes Workload force_app_install Clean install the VM application no Workload Apart from the variables defined, there are three other variables that controls the setup, multipass_vms - defines a dictionary of VMs that needs to be created, multipass_vms : # the name of the VM - name : mgmt # cpus to allocate cpus : 4 # memory to allocate mem : 8g # disk size disk : 30g # roles of this vm role : - kubernetes - gloo - management - name : cluster1 cpus : 4 mem : 8g disk : 30g role : - kubernetes - gloo - workload - name : vm1 cpus : 2 mem : 2g disk : 30g role : - vm gloo_clusters - the Kubernetes clusters that wil be used for gloo deployment gloo_clusters : # name of the cluster mgmt : # cloud where its deployed cloud : k3s # the Kubernetes Context name, recommended it to be the name of VM where k3s runs k8s_context : mgmt # logical cluster name to be used while registering it with meshctl cluster_name : mgmt cluster1 : cloud : k3s k8s_context : cluster1 cluster_name : cluster1 istio_clusters - the Kubernetes clusters where Istio will be deployed istio_clusters : # name of the cluster cluster1 : # Kubernetes Context to use for this cluster k8s_context : \"{{ gloo_clusters.cluster1.k8s_context }}\" # The version of Istio that needs to be deployed version : \"{{ lookup('env','ISTIO_VERSION') }}\" install : yes Tip The demo uses asdf-vm to handle multiple versions of a software e.g. Python, Istio. Check out https://github.com/kameshsampath/asdf-istio The setup uses direnv and the playbooks generates the .envrc using template form $DEMO_HOME/templates/.envrc . If needed adjust the .envrc template and rerun the create-vms and create-kubernetes-clusters task to refresh or update it.","title":"Ansible Variables"},{"location":"setup/#create-virtual-machines","text":"For the demo we will be using multipass to create and run virtual machines. Run the following command to create the virtual machines, make create-vms The previous command would have created three VMs namely, mgmt - which will act as Gloo Management Kubernetes cluster. cluster1 - The Kubernetes cluster where Istio and its workloads will be deployed. The Virtual Machine workloads will use the Istio Control Plane( CP ) opn this cluster for its services. vm1 - the Virtual Machine which will hold a small workload that will be connected to cluster1 . You can always get the information about the multipass VM using the command, multipass info <vm name> # e.g. multipass info cluster1 That should give an information like, Name: cluster1 State: Running IPv4: 192.168.64.90 172.16.0.0 Release: Ubuntu 20.04.3 LTS Image hash: 8fbc4e8c6e33 (Ubuntu 20.04 LTS) Load: 2.37 1.88 1.76 Disk usage: 5.3G out of 28.9G Memory usage: 1.8G out of 7.8G Mounts: -- The task finally generates Ansible Hosts inventory based on the template from $DEMO_HOME/templates/hosts.j2 , which will be used as inventory in other playbook runs.","title":"Create Virtual Machines"},{"location":"setup/#setup-kubernetes-clusters","text":"As part of this demo we will be setting up k3s Kubernetes clusters. The k3s clusters will be a single node cluster run via multipass VM . We will configure that to with the following flags, --cluster-cidr=172.16.0.0/24 allows us to create 65 \u2013 110 Pods on this node --service-cidr=172.18.0.0/20 allows us to create 4096 services --disable=traefik disable traefik deployment Check the GKE doc 1 for a reference on how to calculate the number of pods and service with given CIDR range. Run the following command to deploy the clusters to our mgmt and cluster1 VMs. make create-kubernetes-clusters The previous step we should have two Kubernetes clusters mgmt and cluster1 and as convenience it merges the two cluster kubeconfig into one as $DEMO_HOME/work/.kube/config which is set as the current shell $KUBECONFIG value. So doing kubectl config get-contexts now should return you two contexts. CURRENT NAME CLUSTER AUTHINFO NAMESPACE cluster1 cluster1 cluster1 * mgmt mgmt mgmt Note The k3s deployment as part of this demo will be using Calico which enables us to define routes to the Kubernetes services/pods via its host .","title":"Setup Kubernetes Clusters"},{"location":"setup/#setup-gloo","text":"Let us setup on the mgmt cluster. The setup uses the Gloo Enterprise License, if you don\u2019t have one please request 30 day trial one via solo.io . Set the License key via as $GLOO_MESH_GATEWAY_LICENSE_KEY environment variable and then run the following command to deploy Gloo Mesh, make deploy-gloo Ensure Gloo mesh is setup correctly, meshctl check server Gloo Mesh Management Cluster Installation -------------------------------------------- \ud83d\udfe2 Gloo Mesh Pods Status +----------+------------+-------------------------------+-----------------+ | CLUSTER | REGISTERED | DASHBOARDS AND AGENTS PULLING | AGENTS PUSHING | +----------+------------+-------------------------------+-----------------+ | cluster1 | true | 2 | 1 | +----------+------------+-------------------------------+-----------------+ \ud83d\udfe2 Gloo Mesh Agents Connectivity Management Configuration --------------------------- \ud83d\udfe2 Gloo Mesh CRD Versions \ud83d\udfe2 Gloo Mesh Networking Configuration Resources","title":"Setup Gloo"},{"location":"setup/#setup-istio","text":"Lets now complete the environment setup part by deploying Istio on to cluster1 . We use Istio 1.11.5 for this tutorial, make deploy-istio Lets check if Istio setup is done correctly, istioctl verify-install --context = $CLUSTER1 The command should show the following output ( trimmed for brevity), ... Checked 13 custom resource definitions Checked 1 Istio Deployments \u2714 Istio is installed and verified successfully https://cloud.google.com/kubernetes-engine/docs/concepts/alias-ips \u21a9","title":"Setup Istio"},{"location":"troubleshooting/","text":"Certificates Expired \u00b6 Issue \u00b6 Calling customer service curl $SVC_URL shows an error like, upstream connect error or disconnect/reset before headers. reset reason: connection failure, transport failure reason: TLS error: 268436501 :SSL routines:OPENSSL_internal:SSLV3_ALERT_CERTIFICATE_EXPIRED And hecking ingressgateway logs, kubectl --context = $CLUSTER1 logs -f -listio = ingressgateway -nistio-gateways ingressgateway-7cc46d9596-v8qnt istio-proxy [ 2022 -01-11T06:44:07.918Z ] \"GET /customer HTTP/1.1\" 503 UF,URX upstream_reset_before_response_started { connection_failure,TLS_error:_268436501:SSL_routines:OPENSSL_internal:SSLV3_ALERT_CERTIFICATE_EXPIRED } - \"TLS error: 268436501:SSL routines:OPENSSL_internal:SSLV3_ALERT_CERTIFICATE_EXPIRED\" 0 201 77 - \"192.168.64.90\" \"curl/7.77.0\" \"51c9ed69-6ad3-43a4-a183-58229a34d444\" \"192.168.64.90\" \"172.16.0.20:8080\" outbound | 8080 | v1 | customer.default.svc.cluster.local - 172 .16.0.12:8080 192 .168.64.90:19213 - - Solution \u00b6 Restart Istio and Gateway deployments kubectl --context = $CLUSTER1 rollout restart -n istio-system deployment istiod-1-11-5 kubectl --context = $CLUSTER1 rollout restart -n istio-gateways deployment ingressgateway","title":"Troubleshooting"},{"location":"troubleshooting/#certificates-expired","text":"","title":"Certificates Expired"},{"location":"troubleshooting/#issue","text":"Calling customer service curl $SVC_URL shows an error like, upstream connect error or disconnect/reset before headers. reset reason: connection failure, transport failure reason: TLS error: 268436501 :SSL routines:OPENSSL_internal:SSLV3_ALERT_CERTIFICATE_EXPIRED And hecking ingressgateway logs, kubectl --context = $CLUSTER1 logs -f -listio = ingressgateway -nistio-gateways ingressgateway-7cc46d9596-v8qnt istio-proxy [ 2022 -01-11T06:44:07.918Z ] \"GET /customer HTTP/1.1\" 503 UF,URX upstream_reset_before_response_started { connection_failure,TLS_error:_268436501:SSL_routines:OPENSSL_internal:SSLV3_ALERT_CERTIFICATE_EXPIRED } - \"TLS error: 268436501:SSL routines:OPENSSL_internal:SSLV3_ALERT_CERTIFICATE_EXPIRED\" 0 201 77 - \"192.168.64.90\" \"curl/7.77.0\" \"51c9ed69-6ad3-43a4-a183-58229a34d444\" \"192.168.64.90\" \"172.16.0.20:8080\" outbound | 8080 | v1 | customer.default.svc.cluster.local - 172 .16.0.12:8080 192 .168.64.90:19213 - -","title":"Issue"},{"location":"troubleshooting/#solution","text":"Restart Istio and Gateway deployments kubectl --context = $CLUSTER1 rollout restart -n istio-system deployment istiod-1-11-5 kubectl --context = $CLUSTER1 rollout restart -n istio-gateways deployment ingressgateway","title":"Solution"}]}